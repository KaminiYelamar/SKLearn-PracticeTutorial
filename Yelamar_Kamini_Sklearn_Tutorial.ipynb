{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4026be",
   "metadata": {},
   "source": [
    "# What is Sklearn ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26b5ac",
   "metadata": {},
   "source": [
    "**Scikit-learn (Sklearn)** is Python's most useful and robust machine learning library. It offers a set of efficient tools for machine learning and statistical modeling, such as classification, regression, clustering, and dimensionality reduction, via a Python interface. This predominantly Python-written package is based on NumPy, SciPy, and Matplotlib.\n",
    "\n",
    "Sklearn consists of different packages :\n",
    "1. Classification\n",
    "2. Regression\n",
    "3. Clustering\n",
    "4. Dimensionality reduction\n",
    "5. Model selection\n",
    "6. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f16de8",
   "metadata": {},
   "source": [
    "# Sklearn preprocessing Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd826d",
   "metadata": {},
   "source": [
    "**Pre-processing** refers to the tranformations performed on the data , before sending it to the algorithm. \n",
    "In python, scikit-learn library includes an existing in-built functionality under sklearn.preprocessing module.\n",
    "\n",
    "The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. This module includes scaling, centering, normalization, binarization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f50f9",
   "metadata": {},
   "source": [
    "### Table of Contents :\n",
    "\n",
    "* <a href='#install'>Installation</a>\n",
    "* <a href='#import'>Importing sklearn preprocessing module</a>\n",
    "* <a href='#std'>1.Standardization</a>\n",
    "    * <a href='#std1'>1.1 Scaling Features to a range</a>\n",
    "    * <a href='#std2'>1.2 Scaling Sparse Data </a>\n",
    "    * <a href='#std3'>1.3 Scaling data with Outliers</a>\n",
    "    * <a href='#std4'>1.4 Centering Kernel Matrices</a>\n",
    "* <a href='#norm'>2.Normalization</a>    \n",
    "* <a href='#non-linear'>3.Non-Linear Transformation</a>\n",
    "    * <a href='#non-linear1'>3.1 Mapping to Uniform Distribution (Quantile Transforms)</a>\n",
    "    * <a href='#non-linear2'>3.2 Mapping to Gaussian Distribution (Power Transforms)</a>\n",
    "* <a href='#encode'>4.Encoding</a>\n",
    "* <a href='#polynomial'>5.Polynomial Features</a>\n",
    "    * <a href='#polynomial1'>5.1 Polynomial Features</a>\n",
    "    * <a href='#polynomial2'>5.2 Spline Transformers</a>\n",
    "* <a href='#custom'>6.Custom Transformers</a>\n",
    "* <a href='#ref'>References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f6653",
   "metadata": {},
   "source": [
    "<a id='install'></a>\n",
    "# Installation of Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd1cf6",
   "metadata": {},
   "source": [
    "If we have already installed NumPy and Scipy, two easiest ways to install scikit-learn are:\n",
    "1. Using pip</br>\n",
    "$pip install -U scikit-learn \n",
    "\n",
    "2. Using conda </br>\n",
    "$conda install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e749a",
   "metadata": {},
   "source": [
    "*And, if NumPy and Scipy is not yet installed on your Python workstation then, you can install them by using either pip or conda. (prereq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e80d7",
   "metadata": {},
   "source": [
    "<a id='import'></a>\n",
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e8ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing as skp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263cd562",
   "metadata": {},
   "source": [
    "<a id='std'></a>\n",
    "# 1. Standardization\n",
    "Many machine learning estimators used in scikit-learn require dataset standardization; if the individual features do not more or less resemble standard normally distributed data, they may behave improperly. Standardization is also addressed as mean removal or variance scaling\n",
    "\n",
    "In practice, we often overlook the distribution's structure and simply convert the data to center it by deleting the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "Many aspects in a learning algorithm's objective function, for example, may assume that all features are centered around zero or have variance in the same order. If one variable has a variance that is orders of magnitude greater than others, it may dominate the objective function and prevent the estimator from learning from other features as predicted.\n",
    "\n",
    "The preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206a66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = skp.StandardScaler().fit(X_train)\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9187ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Scalar: [1.         0.         0.33333333]\n"
     ]
    }
   ],
   "source": [
    "print('Mean of Scalar:',scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2c39b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale:  [0.81649658 0.81649658 1.24721913]\n"
     ]
    }
   ],
   "source": [
    "print('Scale: ',scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26028787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final scaled output:\n",
      " [[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n"
     ]
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_train)\n",
    "print('Final scaled output:\\n',X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f4d2b",
   "metadata": {},
   "source": [
    "*Scaled data has mean value as zero and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01bc2cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Scaled data: [0. 0. 0.]\n",
      "Variance of scaled data: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Mean of Scaled data:',X_scaled.mean(axis=0))\n",
    "print('Variance of scaled data:',X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b5859",
   "metadata": {},
   "source": [
    "It is possible to disable either centering or scaling by either passing **'with_mean=False'** or **'with_std=False'** to the constructor of StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b391ffa",
   "metadata": {},
   "source": [
    "Some methods of standardization:\n",
    "1. Scaling features to a range\n",
    "2. Scaling sparse data\n",
    "3. Scaling data with outliers\n",
    "4. Centering kernel matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd0f88d",
   "metadata": {},
   "source": [
    "<a id='std1'></a>\n",
    "### 1.1 Scaling features to a range\n",
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. **MinMaxScaler** or **MaxAbsScaler** can be used to achieve this. </br>\n",
    "</br>\n",
    "The reasons for using this scaling include feature robustness to very small standard deviations and the retention of zero entries in sparse data (a variable in which the cells do not contain actual data within data analysis).</br></br>\n",
    "For Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271ce51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the resultant matrix using MinMaxScaler:\n",
      " [[0.5        0.         1.        ]\n",
      " [1.         0.5        0.33333333]\n",
      " [0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_train2 = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "min_max_scaler = skp.MinMaxScaler()\n",
    "X_train2_minmax = min_max_scaler.fit_transform(X_train2)\n",
    "print('Here\\'s the resultant matrix using MinMaxScaler:\\n', X_train2_minmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f36051",
   "metadata": {},
   "source": [
    "The same transformer instance can then be used to some additional test data that was not seen during the fit call: the same scaling and shifting operations will be used to be consistent with the transformation that was performed on the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4ab8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New matrix using the same transformation as X_train data:\n",
      " [[-1.5         0.          1.66666667]]\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[-3., -1.,  4.]])\n",
    "X_new_minmax = min_max_scaler.transform(X_new)\n",
    "print('New matrix using the same transformation as X_train data:\\n', X_new_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd542f4",
   "metadata": {},
   "source": [
    "Let's introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee98c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale of Transformation:  [0.5        0.5        0.33333333]\n",
      "Minimum of Transformation:  [0.         0.5        0.33333333]\n"
     ]
    }
   ],
   "source": [
    "print('Scale of Transformation: ', min_max_scaler.scale_)\n",
    "\n",
    "print('Minimum of Transformation: ', min_max_scaler.min_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da2273",
   "metadata": {},
   "source": [
    "If **MinMaxScaler** is given an explicit feature_range=(min, max) the full formula is:</br>\n",
    "\n",
    "*X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))*\n",
    "\n",
    "*X_scaled = X_std * (max - min) + min*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb625d7",
   "metadata": {},
   "source": [
    "**MaxAbsScaler** works in a similar manner, but scales the training data so that it falls within the range [-1, 1] by dividing across the biggest maximum value in each feature. It is intended for data that is already centered at zero, as well as sparse data.</br>\n",
    "Above example using MaxAbsScaler scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11358b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix:\n",
      " [[ 0.5 -1.   1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -0.5]]\n",
      "New matrix using trained transformation:\n",
      " [[-1.5 -1.   2. ]]\n",
      "Scale: [2. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "max_abs_scaler = skp.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "print('Training Matrix:\\n', X_train_maxabs)\n",
    "\n",
    "X_new = np.array([[ -3., -1.,  4.]])\n",
    "X_new_maxabs = max_abs_scaler.transform(X_new)\n",
    "print('New matrix using trained transformation:\\n',X_new_maxabs)\n",
    "\n",
    "print('Scale:',max_abs_scaler.scale_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca74fa5",
   "metadata": {},
   "source": [
    "<a id='std2'></a>\n",
    "### 1.2 Scaling Sparse data\n",
    "Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.\n",
    "MaxAbsScaler was specifically designed for scaling sparse data.</br></br>\n",
    "If the centered data is expected to be small enough, explicitly converting the input to an array using the toarray method of sparse matrices is another option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eb21a",
   "metadata": {},
   "source": [
    "<a id='std3'></a>\n",
    "### 1.3 Scaling data with outliers\n",
    "If data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, we can use RobustScaler as a drop-in replacement instead. It uses more robust estimates for the center and range of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b7565",
   "metadata": {},
   "source": [
    "<a id='std4'></a>\n",
    "### 1.4 Centering Kernel matrices\n",
    "If we have a kernel matrix of a kernel \n",
    " that computes a dot product in a feature space (possibly implicitly) defined by a function \n",
    ", a KernelCenterer can transform the kernel matrix so that it contains inner products in the feature space defined by \n",
    " followed by the removal of the mean in that space. In other words, KernelCenterer computes the centered Gram matrix associated to a positive semidefinite kernel ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ab4b3",
   "metadata": {},
   "source": [
    "<a id='norm'></a>\n",
    "# 2. Normalization\n",
    "**Normalization** is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "This assumption is the base of the Vector Space Model often used in text classification and clustering contexts.\n",
    "\n",
    "The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the **l1**, **l2**, or **max norms**:</br>\n",
    "Note: *L2 normalization is also known as **spatial sign preprocessing**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "094a9458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = skp.normalize(X, norm='l2')\n",
    "\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0f154",
   "metadata": {},
   "source": [
    "The preprocessing module further provides a utility class Normalizer that implements the same operation using the Transformer API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c367f447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = skp.Normalizer().fit(X)  # fit does nothing\n",
    "normalizer.transform([[-1.,  1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39470bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40b22c",
   "metadata": {},
   "source": [
    "<a id='non-linear'></a>\n",
    "# 3. Non-Linear Transformation\n",
    "Two types of transformations are : \n",
    "1. Quantile transforms \n",
    "2. Power transforms\n",
    "\n",
    "Both the transforms are based on monotonic transformations of the features, which maintain the rank of the values along each feature.\n",
    "\n",
    "**Quantile transforms** put all features into the same desired distribution whereas **Power transforms** are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ccf94",
   "metadata": {},
   "source": [
    "<a id='non-linear1'></a>\n",
    "### 3.1 Mapping to Uniform Distribution (Quantile Transforms) \n",
    "QuantileTransformer provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a75387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksyel\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (112). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.3, 5.1, 5.8, 6.5, 7.9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading iris data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#Quantile transformer\n",
    "quantile_transformer = skp.QuantileTransformer(random_state=0)\n",
    "\n",
    "#Fitting\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "\n",
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ce5c5",
   "metadata": {},
   "source": [
    "This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17119816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_Trans: [0.         0.23873874 0.50900901 0.74324324 1.        ]\n",
      "X_test: [4.4   5.125 5.75  6.175 7.3  ]\n",
      "X_test_trans [0.01351351 0.25       0.47747748 0.60472973 0.94144144]\n"
     ]
    }
   ],
   "source": [
    "print('X_train_Trans:',np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100]))\n",
    "print('X_test:',np.percentile(X_test[:, 0], [0, 25, 50, 75, 100]))\n",
    "print('X_test_trans',np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bbb0a",
   "metadata": {},
   "source": [
    "<a id='non-linear2'></a>\n",
    "### 3.2 Mapping to Gaussian Distribution (Power Transforms)\n",
    "The normalcy of the features in a dataset is desirable in many modeling scenarios. Power transforms are a type of parametric, monotonic transformation that aims to map data from any distribution to a Gaussian distribution as closely as feasible in order to stabilize variance and decrease skewness.\n",
    "\n",
    "PowerTransformer presently supports two such power transformations: the Yeo-Johnson and Box-Cox transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2706577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      " [[1.28331718 1.18092228 0.84160269]\n",
      " [0.94293279 1.60960836 0.3879099 ]\n",
      " [1.35235668 0.21715673 1.09977091]]\n",
      "\n",
      "Result after mapping:\n",
      " [[ 0.49024349  0.17881995 -0.1563781 ]\n",
      " [-0.05102892  0.58863195 -0.57612414]\n",
      " [ 0.69420009 -0.84857822  0.10051454]]\n"
     ]
    }
   ],
   "source": [
    "pt = skp.PowerTransformer(method='box-cox', standardize=False)\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n",
    "print('Matrix:\\n',X_lognormal)\n",
    "\n",
    "\n",
    "print('\\nResult after mapping:\\n',pt.fit_transform(X_lognormal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1f46f",
   "metadata": {},
   "source": [
    "In above example we have set the **standardize option to False**, PowerTransformer will apply **zero-mean, unit-variance normalization** to the transformed output by default.\n",
    "\n",
    "It is also possible to map data to a normal distribution using QuantileTransformer by setting **output_distribution='normal'**. Using previous example from the iris dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25faf68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksyel\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (150). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.3, 2. , 1. , 0.1],\n",
       "       [4.4, 2.2, 1.1, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.5, 2.3, 1.3, 0.1],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.7, 0.4],\n",
       "       [5.3, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.9, 0.5],\n",
       "       [5.4, 2.9, 1.9, 0.6],\n",
       "       [5.4, 2.9, 3. , 1. ],\n",
       "       [5.4, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.6, 1. ],\n",
       "       [5.5, 2.9, 3.7, 1. ],\n",
       "       [5.5, 3. , 3.8, 1.1],\n",
       "       [5.5, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.7, 3. , 4. , 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.4],\n",
       "       [5.8, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.2, 4.7, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.3, 5. , 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.6, 3.4, 5.4, 1.9],\n",
       "       [6.6, 3.4, 5.4, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.9, 3.6, 5.7, 2.2],\n",
       "       [6.9, 3.6, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.3],\n",
       "       [7. , 3.7, 5.9, 2.3],\n",
       "       [7.1, 3.8, 5.9, 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6.1, 2.3],\n",
       "       [7.3, 3.8, 6.1, 2.3],\n",
       "       [7.4, 3.8, 6.1, 2.3],\n",
       "       [7.6, 3.9, 6.3, 2.4],\n",
       "       [7.7, 3.9, 6.4, 2.4],\n",
       "       [7.7, 4. , 6.6, 2.4],\n",
       "       [7.7, 4.1, 6.7, 2.5],\n",
       "       [7.7, 4.2, 6.7, 2.5],\n",
       "       [7.9, 4.4, 6.9, 2.5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_transformer = skp.QuantileTransformer(\n",
    "    output_distribution='normal', random_state=0)\n",
    "X_trans = quantile_transformer.fit_transform(X)\n",
    "quantile_transformer.quantiles_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95f10f",
   "metadata": {},
   "source": [
    "In above the median of the input becomes the mean of the output, centered at 0.The normal output is clipped so that the input’s minimum and maximum do not become infinite under the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fadf4",
   "metadata": {},
   "source": [
    "<a id='encode'></a>\n",
    "# 4. Encoding categorical values/features\n",
    "Often features are not given as continuous values but categorical. For example a person could have features [\"student\", \"teacher\"], [\"in UNT\", \"in UTA\", \"in UT\"], [\"uses Jupyter Notebook\", \"uses VSCode\", \"uses Colab\", \"uses PyCharm\"]. Such features can be efficiently coded as integers, for instance [\"student\", \"in UTA\", \"uses Jupyter Notebook\"] could be expressed as [0, 1, 0] while [\"teacher\", \"in UT\", \"uses PyCharm\"] would be [1, 2, 3].\n",
    "\n",
    "To convert categorical features to such integer codes, we can use the OrdinalEncoder. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf09c950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = skp.OrdinalEncoder()\n",
    "X = [['student', 'in UTA', 'uses Jupyter Notebook'], ['teacher', 'in UT', 'uses PyCharm']]\n",
    "enc.fit(X)\n",
    "\n",
    "enc.transform([['teacher', 'in UTA', 'uses Jupyter Notebook']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c84dc",
   "metadata": {},
   "source": [
    "By default, OrdinalEncoder will also passthrough missing values that are indicated by np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e72ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [nan],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = skp.OrdinalEncoder()\n",
    "X = [['student'], ['teacher'], [np.nan], ['teacher']]\n",
    "enc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8cbceb",
   "metadata": {},
   "source": [
    "Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6d4118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = skp.OneHotEncoder()\n",
    "X = [['student', 'in UTA', 'uses Jupyter Notebook'], ['teacher', 'in UT', 'uses PyCharm']]\n",
    "enc.fit(X)\n",
    "\n",
    "enc.transform([['teacher', 'in UT', 'uses Jupyter Notebook'],\n",
    "               ['student', 'in UTA', 'uses PyCharm']]).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0a57c",
   "metadata": {},
   "source": [
    "It is possible to specify categories explicitly using the parameter categories. There are two designations, four possible universities and four web tools in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "680eecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders = ['student', 'teacher']\n",
    "locations = ['in UT', 'in UTA', 'in TAMU', 'in UNT']\n",
    "browsers = ['uses Jupyter Notebook', 'uses PyCharm', 'uses Colab', 'uses VSCode']\n",
    "enc = skp.OneHotEncoder(categories=[genders, locations, browsers])\n",
    "# Note that for there are missing categorical values for the 2nd and 3rd\n",
    "# feature\n",
    "X = [['teacher', 'in UNT', 'uses PyCharm'], ['student', 'in UTA', 'uses Jupyter Notebook']]\n",
    "enc.fit(X)\n",
    "enc.transform([['student', 'in TAMU', 'uses VSCode']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1cfd3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [['student', 'in UTA', 'uses VSCode'],\n",
    "     ['teacher', 'in TAMU', 'uses PyCharm']]\n",
    "drop_enc = skp.OneHotEncoder(drop='first').fit(X)\n",
    "drop_enc.categories_\n",
    "\n",
    "\n",
    "drop_enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29356304",
   "metadata": {},
   "source": [
    "<a id='polynomial'></a>\n",
    "# 5. Generating Polynomial Features\n",
    "It is frequently advantageous to add complexity to a model by taking nonlinear aspects of the input data into account. We have two options, both of which are based on polynomials:\n",
    "The first employs pure polynomials, while the second uses splines, which are piecewise polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77e83c",
   "metadata": {},
   "source": [
    "<a id='polynomial1'></a>\n",
    "### 5.1 Polynomial Features\n",
    "Polynomial features are a straightforward and widespread way for obtaining the high-order and interaction terms of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56e5d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix: [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "print('Matrix:',X)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d44ddc",
   "metadata": {},
   "source": [
    "The features of X have been transformed from (X1,X2) to (1, X1, X2, X1^2, X1, X2, X2^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e92ca4",
   "metadata": {},
   "source": [
    "<a id='polynomial2'></a>\n",
    "### 5.2 Spline Transformer\n",
    "Another method for include nonlinear factors in place of pure polynomials of features is to use the SplineTransformer to construct spline basis functions for each feature. Splines are piecewise polynomials that are parametrized by their polynomial degree and knot places. \n",
    "\n",
    "Note: The SplineTransformer treats each feature separately, i.e. it won’t give us interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b82eb2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix: [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "Result after Transformation:\n",
      " [[0.5   0.5   0.    0.   ]\n",
      " [0.125 0.75  0.125 0.   ]\n",
      " [0.    0.5   0.5   0.   ]\n",
      " [0.    0.125 0.75  0.125]\n",
      " [0.    0.    0.5   0.5  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "X = np.arange(5).reshape(5, 1)\n",
    "print('Matrix:',X)\n",
    "\n",
    "spline = SplineTransformer(degree=2, n_knots=3)\n",
    "print('Result after Transformation:\\n',spline.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5ec88",
   "metadata": {},
   "source": [
    "<a id='custom'></a>\n",
    "# 6. Custom Transformers\n",
    "To help with data cleansing or processing, we may want to turn an existing Python function into a transformer. FunctionTransformer allows us to create a transformer from any function. For example, to create a pipeline transformer that does a log transformation, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a9cf3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Matrix:\n",
      " [[0.         0.69314718]\n",
      " [1.09861229 1.38629436]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "\n",
    "# Since FunctionTransformer is no-op during fit, we can call transform directly\n",
    "print('Transformed Matrix:\\n',transformer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697702f2",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "https://www.tutorialspoint.com/ </br>\n",
    "https://www.google.com/ </br>\n",
    "https://scikit-learn.org/ </br>\n",
    "https://pypi.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
